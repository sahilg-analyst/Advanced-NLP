# Load model and create Doc object
nlp = spacy.load('en_core_web_sm')
doc = nlp(blog)

# Generate lemmatized tokens
lemmas = [token.lemma_ for token in doc]

# Remove stopwords and non-alphabetic tokens
a_lemmas = [lemma for lemma in lemmas 
            if lemma.isalpha() and lemma not in stopwords]

# Print string after text cleaning
print(' '.join(a_lemmas))

##token.lemma_ will give you the lemma of each token.
lemma.isalpha() removes tokens which have non-alphabetic characters.

##Take a look at the cleaned text; it is lowercased and devoid of numbers, punctuations and commonly used stopwords. 
Also, note that the word U.S. was present in the original text. Since it had periods in between, our text cleaning process completely removed it. 
This may not be ideal behavior. It is always advisable to use your custom functions in place of isalpha() for more nuanced cases.
